{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fef226c-e266-4e40-9c6e-9958306ccfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-m3\n",
      "INFO:financerag.retrieval.dense:Encoding queries...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 128\n",
      "Working on ConvfinQA Task\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05828da2facb43059d6c490a153dd6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Sorting corpus by document length...\n",
      "INFO:financerag.retrieval.dense:Encoding corpus in batches... This may take a while.\n",
      "INFO:financerag.retrieval.dense:Encoding batch 1/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb522ddcd72c4598945c06e1ca69dd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.33 GiB. GPU 0 has a total capacity of 23.70 GiB of which 887.56 MiB is free. Process 18536 has 6.45 GiB memory in use. Process 4552 has 16.38 GiB memory in use. Of the allocated memory 14.95 GiB is allocated by PyTorch, and 17.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m###################################################################################\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Run Retrieval\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorking on ConvfinQA Task\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m convfinqa_result \u001b[38;5;241m=\u001b[39m convfinqa_task\u001b[38;5;241m.\u001b[39mretrieve(\n\u001b[1;32m     46\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mretrieval_model,\n\u001b[1;32m     47\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(convfinqa_task\u001b[38;5;241m.\u001b[39mcorpus)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.6\u001b[39m))\n\u001b[1;32m     48\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorking on FinBench Task\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/DSC/Financerag/financerag/tasks/Base_Task.py:153\u001b[0m, in \u001b[0;36mBaseTask.retrieve\u001b[0;34m(self, retriever, top_k, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData has not been loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m#top_k = 20000\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# if len(self.corpus)>10000:\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m#     top_k = 3000\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# elif len(self.corpus)>10:\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m#     top_k = int(0.8*len(self.corpus))\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_results \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39mretrieve(\n\u001b[1;32m    154\u001b[0m     queries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueries, corpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus, top_k\u001b[38;5;241m=\u001b[39mtop_k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    155\u001b[0m )\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_results\n",
      "File \u001b[0;32m/data/DSC/Financerag/financerag/retrieval/dense.py:186\u001b[0m, in \u001b[0;36mDenseRetrieval.retrieve\u001b[0;34m(self, corpus, queries, top_k, score_function, return_sorted, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m end_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_chunk_size, \u001b[38;5;28mlen\u001b[39m(corpus_list))\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Encode chunk of corpus\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m sub_corpus_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencode_corpus(\n\u001b[1;32m    187\u001b[0m     corpus_list[start_idx:end_idx], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    188\u001b[0m )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Compute similarities using either cosine similarity or dot product\u001b[39;00m\n\u001b[1;32m    191\u001b[0m cos_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_functions[score_function](\n\u001b[1;32m    192\u001b[0m     query_embeddings, sub_corpus_embeddings\n\u001b[1;32m    193\u001b[0m )\n",
      "File \u001b[0;32m/data/DSC/Financerag/financerag/retrieval/sent_encoder.py:67\u001b[0m, in \u001b[0;36mSentenceTransformerEncoder.encode_corpus\u001b[0;34m(self, corpus, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_prompt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_prompt \u001b[38;5;241m+\u001b[39m s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_model\u001b[38;5;241m.\u001b[39mencode(sentences, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/DSC/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:601\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    603\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m/opt/conda/envs/DSC/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:668\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    667\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/DSC/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/DSC/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/DSC/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:118\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    116\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    119\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    121\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m/opt/conda/envs/DSC/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/DSC/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/DSC/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:943\u001b[0m, in \u001b[0;36mXLMRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    936\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_causal_attention_mask_for_sdpa(\n\u001b[1;32m    937\u001b[0m             attention_mask,\n\u001b[1;32m    938\u001b[0m             input_shape,\n\u001b[1;32m    939\u001b[0m             embedding_output,\n\u001b[1;32m    940\u001b[0m             past_key_values_length,\n\u001b[1;32m    941\u001b[0m         )\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[1;32m    944\u001b[0m             attention_mask, embedding_output\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39mseq_length\n\u001b[1;32m    945\u001b[0m         )\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)\n",
      "File \u001b[0;32m/opt/conda/envs/DSC/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:447\u001b[0m, in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AttentionMaskConverter\u001b[38;5;241m.\u001b[39m_expand_mask(mask\u001b[38;5;241m=\u001b[39mmask, dtype\u001b[38;5;241m=\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39mtgt_len)\n",
      "File \u001b[0;32m/opt/conda/envs/DSC/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:188\u001b[0m, in \u001b[0;36mAttentionMaskConverter._expand_mask\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    184\u001b[0m expanded_mask \u001b[38;5;241m=\u001b[39m mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :]\u001b[38;5;241m.\u001b[39mexpand(bsz, \u001b[38;5;241m1\u001b[39m, tgt_len, src_len)\u001b[38;5;241m.\u001b[39mto(dtype)\n\u001b[1;32m    186\u001b[0m inverted_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m expanded_mask\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inverted_mask\u001b[38;5;241m.\u001b[39mmasked_fill(inverted_mask\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool), torch\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.33 GiB. GPU 0 has a total capacity of 23.70 GiB of which 887.56 MiB is free. Process 18536 has 6.45 GiB memory in use. Process 4552 has 16.38 GiB memory in use. Of the allocated memory 14.95 GiB is allocated by PyTorch, and 17.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch, gc\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from financerag.rerank import CrossEncoderReranker\n",
    "from financerag.retrieval import DenseRetrieval, SentenceTransformerEncoder, BM25Retriever, HybridRetriever\n",
    "from financerag.tasks import ConvFinQA, FinanceBench, FinDER, FinQA, FinQABench, MultiHiertt, TATQA\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "encoding_batch_size=128\n",
    "rerank1_batch_size=32\n",
    "rerank2_batch_size=16\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "###################################################################################\n",
    "convfinqa_task = ConvFinQA()\n",
    "finbench_task = FinanceBench()\n",
    "finder_task = FinDER()\n",
    "finqa_task = FinQA()\n",
    "finqabench_task = FinQABench()\n",
    "multih_task = MultiHiertt()\n",
    "tatqa_task = TATQA()\n",
    "###################################################################################\n",
    "# Encoder & retrieval\n",
    "base_encoder = \"BAAI/bge-m3\" #\"BAAI/bge-m3\" # \"intfloat/multilingual-e5-large-instruct\"  #\"BAAI/bge-large-en-v1.5\" #\"nvidia/NV-Embed-v2\"(20GB) \"intfloat/e5-mistral-7b-instruct\"(9GB) \n",
    "                                            #\"dunzhang/stella_en_1.5B_v5\" (6GB)  \"jinaai/jina-embeddings-v3\"(1.1GB) \"jinaai/jina-embeddings-v2-base-code\"(320MB)\n",
    "encoder_model = SentenceTransformerEncoder(\n",
    "    model_name_or_path=base_encoder,\n",
    "    query_prompt='query: ',\n",
    "    doc_prompt='passage: '\n",
    ")\n",
    "\n",
    "retrieval_model = DenseRetrieval(\n",
    "    model=encoder_model,\n",
    "    batch_size=encoding_batch_size\n",
    " )\n",
    "###################################################################################\n",
    "# Run Retrieval\n",
    "print(\"Working on ConvfinQA Task\")\n",
    "convfinqa_result = convfinqa_task.retrieve(\n",
    "    retriever=retrieval_model,\n",
    "    top_k=int(len(convfinqa_task.corpus)*0.6))\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on FinBench Task\")\n",
    "finbench_result = finbench_task.retrieve(\n",
    "    retriever=retrieval_model,\n",
    "    top_k=int(len(finbench_task.corpus)*0.9))\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on FinDER Task\")\n",
    "finder_result = finder_task.retrieve(\n",
    "    retriever=retrieval_model,\n",
    "    top_k=int(len(finder_task.corpus)*0.3))\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on FinQA Task\")\n",
    "finqa_result = finqa_task.retrieve(\n",
    "    retriever=retrieval_model,\n",
    "    top_k=int(len(finqa_task.corpus)*0.6))\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on FinQABench Task\")\n",
    "finqabench_result = finqabench_task.retrieve(\n",
    "    retriever=retrieval_model,\n",
    "    top_k=int(len(finqabench_task.corpus)*0.8))\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on MultiHiertt Task\")\n",
    "multih_result = multih_task.retrieve(\n",
    "    retriever=retrieval_model,\n",
    "    top_k=int(len(multih_task.corpus)*0.3))\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on TATQA Task\")\n",
    "tatqa_result = tatqa_task.retrieve(\n",
    "    retriever=retrieval_model,\n",
    "    top_k=int(len(tatqa_task.corpus)*0.5))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "results = [\n",
    "    convfinqa_result,\n",
    "    finbench_result,\n",
    "    finder_result,\n",
    "    finqa_result,\n",
    "    finqabench_result,\n",
    "    multih_result,\n",
    "    tatqa_result\n",
    "]\n",
    "###################################################################################\n",
    "def get_evalset(dataset_name):\n",
    "    qrels = {}\n",
    "    df_qrels = pd.read_csv(f\"./data/test/{dataset_name}_qrels.tsv\", sep='\\t')\n",
    "    for _, row in df_qrels.iterrows():\n",
    "        if row['query_id'] not in qrels:\n",
    "            qrels[row['query_id']] = {}\n",
    "        qrels[row['query_id']][row['corpus_id']] = row['score']\n",
    "    return qrels\n",
    "\n",
    "tasks = [\n",
    "    convfinqa_task,\n",
    "    finbench_task,\n",
    "    finder_task,\n",
    "    finqa_task,\n",
    "    finqabench_task,\n",
    "    multih_task,\n",
    "    tatqa_task\n",
    "]\n",
    "\n",
    "qrels = [\n",
    "    get_evalset('ConvFinQA'),\n",
    "    get_evalset('FinanceBench'),\n",
    "    get_evalset('FinDER'),\n",
    "    get_evalset('FinQA'),\n",
    "    get_evalset('FinQABench'),\n",
    "    get_evalset('MultiHeirtt'),\n",
    "    get_evalset('TATQA')\n",
    "]\n",
    "\n",
    "ndcg_values = []\n",
    "map_values = []\n",
    "recall_values = []\n",
    "precision_values = []\n",
    "\n",
    "dataset_names = [\n",
    "    'ConvFinQA',\n",
    "    'FinanceBench',\n",
    "    'FinDER',\n",
    "    'FinQA',\n",
    "    'FinQABench',\n",
    "    'MultiHeirtt',\n",
    "    'TATQA'\n",
    "]\n",
    "\n",
    "for qrel, task in zip(qrels, tasks):\n",
    "    metrics = task.evaluate(qrels=qrel, results=task.retrieve_results, k_values=[10])\n",
    "    ndcg_values.append(metrics[0]['NDCG@10'])  # NDCG@10\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(ndcg_values, marker='o', label='NDCG@10', color='b')\n",
    "plt.title('NDCG@10')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "###################################################################################\n",
    "base_reranker = \"BAAI/bge-reranker-base\" #\"BAAI/bge-reranker-base\" #m2 \n",
    "\n",
    "reranker = CrossEncoderReranker(\n",
    "    model=CrossEncoder(base_reranker))\n",
    "###################################################################################\n",
    "# Step 6: Perform reranking 1\n",
    "batch_size = rerank1_batch_size # 32\n",
    "\n",
    "print(\"\\nWorking on ConvFinQA Reranking\")\n",
    "retrieve_k = len(list(convfinqa_task.retrieve_results.values())[0])\n",
    "top_k = int(0.6 * retrieve_k)\n",
    "convfinqa_rerank = convfinqa_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=convfinqa_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinBench Reranking\")\n",
    "retrieve_k = len(list(finbench_task.retrieve_results.values())[0])\n",
    "top_k = int(0.8 * retrieve_k)\n",
    "finbench_rerank = finbench_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=finbench_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinDER Reranking\")\n",
    "retrieve_k = len(list(finder_task.retrieve_results.values())[0])\n",
    "top_k = int(0.4 * retrieve_k)\n",
    "finder_rerank = finder_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=finder_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinQA Reranking\")\n",
    "retrieve_k = len(list(finqa_task.retrieve_results.values())[0])\n",
    "top_k = int(0.6 * retrieve_k)\n",
    "finqa_rerank = finqa_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=finqa_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinQABench Reranking\")\n",
    "retrieve_k = len(list(finqabench_task.retrieve_results.values())[0])\n",
    "top_k = int(0.8 * retrieve_k)\n",
    "finqabench_rerank = finqabench_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=finqabench_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on MultiHiertt Reranking\")\n",
    "retrieve_k = len(list(multih_task.retrieve_results.values())[0])\n",
    "top_k = int(0.4 * retrieve_k)\n",
    "multih_rerank = multih_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=multih_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on TATQA Reranking\")\n",
    "retrieve_k = len(list(tatqa_task.retrieve_results.values())[0])\n",
    "top_k = int(0.6 * retrieve_k)\n",
    "tatqa_rerank = tatqa_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=tatqa_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "reranking_results = [\n",
    "    convfinqa_rerank,\n",
    "    finbench_rerank,\n",
    "    finder_rerank,\n",
    "    finqa_rerank,\n",
    "    finqabench_rerank,\n",
    "    multih_rerank,\n",
    "    tatqa_rerank\n",
    "]\n",
    "###################################################################################\n",
    "retrieval_ndcg_values = []\n",
    "rerank_ndcg_values = []\n",
    "\n",
    "retrieval_map_values = []\n",
    "rerank_map_values = []\n",
    "\n",
    "retrieval_recall_values = []\n",
    "rerank_recall_values = []\n",
    "\n",
    "retrieval_precision_values = []\n",
    "rerank_precision_values = []\n",
    "\n",
    "# 데이터셋 이름 리스트\n",
    "dataset_names = [\n",
    "    'ConvFinQA',\n",
    "    'FinanceBench',\n",
    "    'FinDER',\n",
    "    'FinQA',\n",
    "    'FinQABench',\n",
    "    'MultiHeirtt',\n",
    "    'TATQA'\n",
    "]\n",
    "\n",
    "for qrel, task in zip(qrels, tasks):\n",
    "    # 평가지표 평가 (retrieval 결과)\n",
    "    #retrieval_metrics = task.evaluate(qrels=qrel, results=task.retrieve_results, k_values=[10])\n",
    "    \n",
    "    # 평가지표 평가 (rerank 결과)\n",
    "    rerank_metrics = task.evaluate(qrels=qrel, results=task.rerank_results, k_values=[10])\n",
    "\n",
    "    # retrieval_ndcg_values.append(retrieval_metrics[0]['NDCG@10'])\n",
    "    # retrieval_map_values.append(retrieval_metrics[1]['MAP@10'])\n",
    "    # retrieval_recall_values.append(retrieval_metrics[2]['Recall@10'])\n",
    "    # retrieval_precision_values.append(retrieval_metrics[3]['P@10'])\n",
    "    \n",
    "    rerank_ndcg_values.append(rerank_metrics[0]['NDCG@10'])\n",
    "    rerank_map_values.append(rerank_metrics[1]['MAP@10'])\n",
    "    rerank_recall_values.append(rerank_metrics[2]['Recall@10'])\n",
    "    rerank_precision_values.append(rerank_metrics[3]['P@10'])\n",
    "\n",
    "# 그래프 생성\n",
    "plt.figure(figsize=(14, 10))\n",
    "#plt.plot(retrieval_ndcg_values, marker='o', label='Retrieval NDCG@10', color='b')\n",
    "plt.plot(rerank_ndcg_values, marker='x', label='Rerank NDCG@10', color='b', linestyle='--')\n",
    "plt.title('NDCG@10 Comparison')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "###################################################################################\n",
    "base_reranker2 = \"BAAI/bge-reranker-v2-m3\" # \"BAAI/bge-reranker-base\" #\"jinaai/jina-reranker-v2-base-multilingual\" #'cross-encoder/ms-marco-MiniLM-L-12-v2' #\"BAAI/bge-reranker-base\"\n",
    "\n",
    "reranker2 = CrossEncoderReranker(\n",
    "    model=CrossEncoder(base_reranker2))\n",
    "###################################################################################\n",
    "# Step 6: Perform reranking\n",
    "\n",
    "top_k= 15 # Number of Reranking results\n",
    "batch_size = rerank2_batch_size # 32\n",
    "\n",
    "print(\"\\nWorking on ConvFinQA Reranking\")\n",
    "convfinqa_rerank_second = convfinqa_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=convfinqa_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinBench Reranking\")\n",
    "finbench_rerank_second = finbench_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=finbench_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinDER Reranking\")\n",
    "finder_rerank_second = finder_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=finder_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinQA Reranking\")\n",
    "finqa_rerank_second = finqa_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=finqa_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinQABench Reranking\")\n",
    "finqabench_rerank_second = finqabench_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=finqabench_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on MultiHiertt Reranking\")\n",
    "multih_rerank_second = multih_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=multih_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on TATQA Reranking\")\n",
    "tatqa_rerank_second = tatqa_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=tatqa_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "reranking_results_second = [\n",
    "    convfinqa_rerank_second,\n",
    "    finbench_rerank_second,\n",
    "    finder_rerank_second,\n",
    "    finqa_rerank_second,\n",
    "    finqabench_rerank_second,\n",
    "    multih_rerank_second,\n",
    "    tatqa_rerank_second\n",
    "]\n",
    "###################################################################################\n",
    "rerank_second_ndcg_values = []\n",
    "\n",
    "dataset_names = [\n",
    "    'ConvFinQA',\n",
    "    'FinanceBench',\n",
    "    'FinDER',\n",
    "    'FinQA',\n",
    "    'FinQABench',\n",
    "    'MultiHeirtt',\n",
    "    'TATQA'\n",
    "]\n",
    "\n",
    "for qrel, task in zip(qrels, tasks):\n",
    "    rerank_second_metrics = task.evaluate(qrels=qrel, results=task.rerank_results, k_values=[10])\n",
    "    rerank_second_ndcg_values.append(rerank_second_metrics[0]['NDCG@10'])\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.plot(retrieval_ndcg_values, marker='o', label='Retrieval NDCG@10', color='b')\n",
    "plt.plot(rerank_ndcg_values, marker='x', label='Rerank NDCG@10', color='b', linestyle='--')\n",
    "plt.plot(rerank_second_ndcg_values, marker='s', label='Rerank Second NDCG@10', color='b', linestyle='-.')\n",
    "plt.title('NDCG@10 Comparison')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "###################################################################################\n",
    "# Step 7: Save results\n",
    "results_dir = './financerag/results/'\n",
    "time_obj = datetime.now()\n",
    "subfolder = \"submission_\" + time_obj.strftime('%m%d%H%M')\n",
    "output_dir = results_dir + subfolder\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "results_df = [\n",
    "    convfinqa_task.load_results(),\n",
    "    finbench_task.load_results(),\n",
    "    finder_task.load_results(),\n",
    "    finqa_task.load_results(),\n",
    "    finqabench_task.load_results(),\n",
    "    multih_task.load_results(),\n",
    "    tatqa_task.load_results()\n",
    "]\n",
    "\n",
    "combined_df = pd.concat(results_df, ignore_index=False)\n",
    "combined_df.to_csv(output_dir + '/' + subfolder + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de4d7d-9a0c-43ab-9e41-23e48c9d7926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01479d11-6782-4d2d-b88e-29297c2eecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a98e536-d8f8-47b6-9d76-fa87589a6826",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Reranker' from 'jina' (/opt/conda/envs/DSC/lib/python3.12/site-packages/jina/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjina\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Reranker\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjina\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentArray, Document\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Reranker' from 'jina' (/opt/conda/envs/DSC/lib/python3.12/site-packages/jina/__init__.py)"
     ]
    }
   ],
   "source": [
    "from jina import Reranker\n",
    "from jina.types.document import DocumentArray, Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41bda830-cd12-4856-b254-9dfcbab51b53",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Reranker' from 'jina' (/opt/conda/envs/DSC/lib/python3.12/site-packages/jina/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjina\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Reranker\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjina\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentArray, Document\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 모델을 로드\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Reranker' from 'jina' (/opt/conda/envs/DSC/lib/python3.12/site-packages/jina/__init__.py)"
     ]
    }
   ],
   "source": [
    "from jina import Reranker\n",
    "from jina.types.document import DocumentArray, Document\n",
    "\n",
    "# 모델을 로드\n",
    "model_name = \"jinaai/jina-reranker-v2-base-multilingual\"\n",
    "reranker = Reranker(model_name=model_name)\n",
    "\n",
    "# 사용할 예제 쿼리와 문서\n",
    "query = \"What is the capital of France?\"\n",
    "documents = [\"Paris is the capital of France.\", \"Berlin is the capital of Germany.\", \"Madrid is the capital of Spain.\"]\n",
    "\n",
    "# 문서 점수 매기기\n",
    "results = reranker.rank(query, documents)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Ranked results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5d67730-4571-4bf7-b750-72d50ae9fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'jinaai/jina-reranker-v2-base-multilingual',\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_flash_attn=False\n",
    ")\n",
    "\n",
    "model.to('cuda') # or 'cpu' if no GPU is available\n",
    "model.eval()\n",
    "\n",
    "# Example query and documents\n",
    "query = \"Organic skincare products for sensitive skin\"\n",
    "documents = [\n",
    "    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n",
    "    \"New makeup trends focus on bold colors and innovative techniques\",\n",
    "    \"Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\",\n",
    "    \"Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\",\n",
    "    \"Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\",\n",
    "    \"Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\",\n",
    "    \"针对敏感肌专门设计的天然有机护肤产品\",\n",
    "    \"新的化妆趋势注重鲜艳的颜色和创新的技巧\",\n",
    "    \"敏感肌のために特別に設計された天然有機スキンケア製品\",\n",
    "    \"新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\",\n",
    "]\n",
    "\n",
    "# construct sentence pairs\n",
    "sentence_pairs = [[query, doc] for doc in documents]\n",
    "\n",
    "scores = model.compute_score(sentence_pairs, max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1a0232-895b-4dba-acd5-de1c3f2fa4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8311430811882019, 0.09401018172502518, 0.6325027346611023, 0.08269733935594559, 0.7620701193809509, 0.09947021305561066, 0.9252299070358276, 0.05921025574207306, 0.8418256044387817, 0.11124119907617569]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9b471-8e93-48f5-b549-78f98c9027a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d5de3-500e-4285-a01c-e3ea34a58ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"submission_12080648_multih\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f07b813-6d5d-4687-baad-00af077bffbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d1793-24d8-4cc8-8681-f50f35c3bbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSC",
   "language": "python",
   "name": "dsc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
