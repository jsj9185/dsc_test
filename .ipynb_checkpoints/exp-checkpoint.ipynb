{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch, gc\n",
    "from importlib import reload\n",
    "\n",
    "import financerag.tasks.Base_Task; reload(financerag.tasks.Base_Task)\n",
    "import financerag.tasks.ConvFinQATask; reload(financerag.tasks.ConvFinQATask)\n",
    "import financerag.tasks.FinanceBenchTask; reload(financerag.tasks.FinanceBenchTask)\n",
    "import financerag.tasks.FinDERTask; reload(financerag.tasks.FinDERTask)\n",
    "import financerag.tasks.FinQATask; reload(financerag.tasks.FinQATask)\n",
    "import financerag.tasks.FinQABenchTask; reload(financerag.tasks.FinQABenchTask)\n",
    "import financerag.tasks.MultiHierttTask; reload(financerag.tasks.MultiHierttTask)\n",
    "import financerag.tasks.TATQATask; reload(financerag.tasks.TATQATask)\n",
    "reload(financerag.tasks)\n",
    "\n",
    "from financerag.rerank import CrossEncoderReranker\n",
    "from financerag.retrieval import DenseRetrieval, SentenceTransformerEncoder, BM25Retriever, HybridRetriever\n",
    "from financerag.tasks import ConvFinQA, FinanceBench, FinDER, FinQA, FinQABench, MultiHiertt, TATQA\n",
    "\n",
    "# Setup basic logging configuration to show info level messages.\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "results_dir = './financerag/results/'\n",
    "time_obj = datetime.now()\n",
    "subfolder = \"submission_\" + time_obj.strftime('%m%d%H%M')\n",
    "output_dir = results_dir + subfolder\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "combined_df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago']\n",
    "})\n",
    "combined_df.to_csv(output_dir + '/' + subfolder + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(1)\n",
    "print(\"Current GPU:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changed\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initialize FinDER Task\n",
    "\n",
    "convfinqa_task = ConvFinQA()\n",
    "finbench_task = FinanceBench()\n",
    "finder_task = FinDER()\n",
    "finqa_task = FinQA()\n",
    "finqabench_task = FinQABench()\n",
    "multih_task = MultiHiertt()\n",
    "tatqa_task = TATQA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13863"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finder_task.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the redemption price for the 0.875% 2025 Notes and the 1.375% 2029 Notes if they are redeemed prior to the applicable Par Call Date?\\n\\nredemption price, 0.875% Notes, 1.375% Notes, Par Call Date'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qu = pd.read_json(\"data/finqabench/queries.jsonl\", lines=True)\n",
    "qu['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the FY2019 - FY2020 total revenue growth rate for Block (formerly known as Square)? Answer in units of percents and round to one decimal place. Approach the question asked by assuming the standpoint of an investment banking analyst who only has access to the statement of income.\\n\\ntotal revenue growth rate, Block, Square'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qu = pd.read_json(\"data/finbench/queries.jsonl\", lines=True)\n",
    "qu['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q4aa0b116</td>\n",
       "      <td></td>\n",
       "      <td>What is the redemption price for the 0.875% 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q4aa0a48c</td>\n",
       "      <td></td>\n",
       "      <td>What is the maturity date of the 0.875% 2025 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q4aa0aeb4</td>\n",
       "      <td></td>\n",
       "      <td>What are the conditions under which the Compan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q4aa05fa4</td>\n",
       "      <td></td>\n",
       "      <td>What is the Company's investment policy and st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q4aa092f8</td>\n",
       "      <td></td>\n",
       "      <td>What is the purpose of the Company's internal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>q4aa01fda</td>\n",
       "      <td></td>\n",
       "      <td>What risks are associated with the Company's i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>q4aa0a1d0</td>\n",
       "      <td></td>\n",
       "      <td>What percentage of outstanding shares must vot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>q4aa0a34c</td>\n",
       "      <td></td>\n",
       "      <td>What is the minimum percentage of shares requi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>q4aa05284</td>\n",
       "      <td></td>\n",
       "      <td>What was the approximate dollar value of share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>q4aa0b3fa</td>\n",
       "      <td></td>\n",
       "      <td>What is the \"Par Call Date\" for the 0.875% 202...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          _id title                                               text\n",
       "0   q4aa0b116        What is the redemption price for the 0.875% 20...\n",
       "1   q4aa0a48c        What is the maturity date of the 0.875% 2025 N...\n",
       "2   q4aa0aeb4        What are the conditions under which the Compan...\n",
       "3   q4aa05fa4        What is the Company's investment policy and st...\n",
       "4   q4aa092f8        What is the purpose of the Company's internal ...\n",
       "..        ...   ...                                                ...\n",
       "95  q4aa01fda        What risks are associated with the Company's i...\n",
       "96  q4aa0a1d0        What percentage of outstanding shares must vot...\n",
       "97  q4aa0a34c        What is the minimum percentage of shares requi...\n",
       "98  q4aa05284        What was the approximate dollar value of share...\n",
       "99  q4aa0b3fa        What is the \"Par Call Date\" for the 0.875% 202...\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qu = pd.read_json(\"data/finqabench/queries_origin.jsonl\", lines=True)\n",
    "qu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 줄임말 확장\n",
    "# 테이블을 instruction: 테이블 컴포넌트를 뽑아달라고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 8\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize DenseRetriever model\n",
    "\n",
    "# You can replace 'intfloat/e5-large-v2' with any other model supported by SentenceTransformers.\n",
    "# For example: 'BAAI/bge-large-en-v1.5', 'Linq-AI-Research/Linq-Embed-Mistral', etc.\n",
    "base_encoder = \"BAAI/bge-base-en-v1.5\" #\"BAAI/bge-m3\" # \"intfloat/multilingual-e5-large-instruct\"  #\"BAAI/bge-large-en-v1.5\" #\"nvidia/NV-Embed-v2\"(20GB) \"intfloat/e5-mistral-7b-instruct\"(9GB) \n",
    "                                            #\"dunzhang/stella_en_1.5B_v5\" (6GB)  \"jinaai/jina-embeddings-v3\"(1.1GB) \"jinaai/jina-embeddings-v2-base-code\"(320MB)\n",
    "encoder_model = SentenceTransformerEncoder(\n",
    "    model_name_or_path=base_encoder,\n",
    "    query_prompt='query: ',\n",
    "    doc_prompt='passage: '\n",
    ")\n",
    "\n",
    "retrieval_model = DenseRetrieval(\n",
    "     model=encoder_model\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Encoding queries...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on FinBench Task\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f8ea9ccfc143f9971892c273c4c3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Sorting corpus by document length...\n",
      "INFO:financerag.retrieval.dense:Encoding corpus in batches... This may take a while.\n",
      "INFO:financerag.retrieval.dense:Encoding batch 1/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c0434c3d7847c0b791a34338e4b24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1733 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Working on FinBench Task\")\n",
    "finder_results = finder_task.retrieve(\n",
    "    retriever=retrieval_model,\n",
    "    top_k = 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "print(len(finder_task.retrieve_results['q00001']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       query_id  corpus_id     score\n",
      "0     qd4982518  dd4c4c208  0.761090\n",
      "1     qd4982518  dd4ba3af4  0.753088\n",
      "2     qd4982518  dd4b89cbc  0.745192\n",
      "3     qd4982518  dd4c4fb38  0.743647\n",
      "4     qd4982518  dd4bf6f9c  0.743375\n",
      "...         ...        ...       ...\n",
      "5205  q4aa0b3fa  dd2b07a56  0.727264\n",
      "5206  q4aa0b3fa  dd2b1453a  0.727212\n",
      "5207  q4aa0b3fa  dd2ad1a32  0.726640\n",
      "5208  q4aa0b3fa  dd2ac1c4a  0.726012\n",
      "5209  q4aa0b3fa  dd2ad662c  0.725589\n",
      "\n",
      "[5210 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# 결과를 DataFrame으로 불러오기\n",
    "results_df = [\n",
    "    convfinqa_task.load_results(),\n",
    "    finbench_task.load_results(),\n",
    "]\n",
    "\n",
    "# DataFrame을 하나로 합치기\n",
    "combined_df = pd.concat(results_df, ignore_index=True)\n",
    "\n",
    "# 결과 확인\n",
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2066, 421)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(convfinqa_task.corpus), len(convfinqa_task.retrieve_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qd4982518'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(convfinqa_task.queries.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_results_ex = convfinqa_task.retrieve_results['qd4982518']\n",
    "conf_results_ex1 = {k: v for k, v in sorted(conf_results_ex.items(), key=lambda item: item[1], reverse=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(convfinqa_task.corpus), len(convfinqa_task.retrieve_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# bm25_retriever = BM25Retriever()\n",
    "# dense_retriever = DenseRetrieval(model=encoder_model)\n",
    "\n",
    "# # Initialize the hybrid retriever\n",
    "# hybrid_retriever = HybridRetriever(\n",
    "#     lexical_retriever=bm25_retriever,\n",
    "#     dense_retriever=dense_retriever,\n",
    "#     lexical_weight=0.4,\n",
    "#     dense_weight=0.6\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Encoding queries...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on ConvfinQA Task\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ccc09a072f49cb8c0d1cf30818284b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Sorting corpus by document length...\n",
      "INFO:financerag.retrieval.dense:Encoding corpus in batches... This may take a while.\n",
      "INFO:financerag.retrieval.dense:Encoding batch 1/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8cfd9d9f0542279249f6b2645603d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Encoding queries...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on FinBench Task\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c639144828400cb7dd32f4d13efee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Sorting corpus by document length...\n",
      "INFO:financerag.retrieval.dense:Encoding corpus in batches... This may take a while.\n",
      "INFO:financerag.retrieval.dense:Encoding batch 1/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b013e9bbf94419fab5a469a5d97ed91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Encoding queries...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on FinDER Task\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8b2ab4b7aa47359ec7d5ec59e2734d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Sorting corpus by document length...\n",
      "INFO:financerag.retrieval.dense:Encoding corpus in batches... This may take a while.\n",
      "INFO:financerag.retrieval.dense:Encoding batch 1/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b38d08e99a44a89b754777b0b84d611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1733 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorking on FinDER Task\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m finder_result \u001b[38;5;241m=\u001b[39m finder_task\u001b[38;5;241m.\u001b[39mretrieve(\n\u001b[1;32m     13\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mretrieval_model)\n\u001b[1;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorking on FinQA Task\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/DSC/Financerag/financerag/tasks/Base_Task.py:152\u001b[0m, in \u001b[0;36mBaseTask.retrieve\u001b[0;34m(self, retriever, top_k, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20000\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# if len(self.corpus)>10000:\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m#     top_k = 5000\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# elif len(self.corpus)>2000:\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m#     top_k = int(0.8*len(self.corpus))\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# elif len(self.corpus)>10:\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m#     top_k = int(0.8*len(self.corpus))\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_results \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39mretrieve(\n\u001b[1;32m    153\u001b[0m     queries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueries, corpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus, top_k\u001b[38;5;241m=\u001b[39mtop_k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    154\u001b[0m )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_results\n",
      "File \u001b[0;32m/data/DSC/Financerag/financerag/retrieval/dense.py:186\u001b[0m, in \u001b[0;36mDenseRetrieval.retrieve\u001b[0;34m(self, corpus, queries, top_k, score_function, return_sorted, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m end_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_chunk_size, \u001b[38;5;28mlen\u001b[39m(corpus_list))\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Encode chunk of corpus\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m sub_corpus_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencode_corpus(\n\u001b[1;32m    187\u001b[0m     corpus_list[start_idx:end_idx], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    188\u001b[0m )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Compute similarities using either cosine similarity or dot product\u001b[39;00m\n\u001b[1;32m    191\u001b[0m cos_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_functions[score_function](\n\u001b[1;32m    192\u001b[0m     query_embeddings, sub_corpus_embeddings\n\u001b[1;32m    193\u001b[0m )\n",
      "File \u001b[0;32m/data/DSC/Financerag/financerag/retrieval/sent_encoder.py:67\u001b[0m, in \u001b[0;36mSentenceTransformerEncoder.encode_corpus\u001b[0;34m(self, corpus, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_prompt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_prompt \u001b[38;5;241m+\u001b[39m s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_model\u001b[38;5;241m.\u001b[39mencode(sentences, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/DSC/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:630\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[1;32m    629\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[0;32m--> 630\u001b[0m                 embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    632\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    634\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 4: Perform retrieval\n",
    "\n",
    "print(\"Working on ConvfinQA Task\")\n",
    "convfinqa_result = convfinqa_task.retrieve(\n",
    "    retriever=retrieval_model)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on FinBench Task\")\n",
    "finbench_result = finbench_task.retrieve(\n",
    "    retriever=retrieval_model)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on FinDER Task\")\n",
    "finder_result = finder_task.retrieve(\n",
    "    retriever=retrieval_model)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on FinQA Task\")\n",
    "finqa_result = finqa_task.retrieve(\n",
    "    retriever=retrieval_model)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on FinQABench Task\")\n",
    "finqabench_result = finqabench_task.retrieve(\n",
    "    retriever=retrieval_model)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on MultiHiertt Task\")\n",
    "multih_result = multih_task.retrieve(\n",
    "    retriever=retrieval_model)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on TATQA Task\")\n",
    "tatqa_result = tatqa_task.retrieve(\n",
    "    retriever=retrieval_model)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "results = [\n",
    "    convfinqa_result,\n",
    "    finbench_result,\n",
    "    finder_result,\n",
    "    finqa_result,\n",
    "    finqabench_result,\n",
    "    multih_result,\n",
    "    tatqa_result\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print a portion of the retrieval results to verify the output.\n",
    "# for result in results:\n",
    "#     print(f\"Retrieved results for {len(result)} queries. Here's an example of the top 5 documents for the first query:\")\n",
    "\n",
    "# for result in results:\n",
    "#     for q_id, res in result.items():\n",
    "#         print(f\"\\nQuery ID: {q_id}\")\n",
    "#         # Sort the result to print the top 5 document ID and its score\n",
    "#         sorted_results = sorted(res.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#         for i, (doc_id, score) in enumerate(sorted_results[:5]):\n",
    "#             print(f\"  Document {i + 1}: Document ID = {doc_id}, Score = {score}\")\n",
    "\n",
    "#         break  # Only show the first query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evalset(dataset_name):\n",
    "    qrels = {}\n",
    "    df_qrels = pd.read_csv(f\"./data/test/{dataset_name}_qrels.tsv\", sep='\\t')\n",
    "    for _, row in df_qrels.iterrows():\n",
    "        if row['query_id'] not in qrels:\n",
    "            qrels[row['query_id']] = {}\n",
    "        qrels[row['query_id']][row['corpus_id']] = row['score']\n",
    "    return qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    convfinqa_task,\n",
    "    finbench_task,\n",
    "    finder_task,\n",
    "    finqa_task,\n",
    "    finqabench_task,\n",
    "    multih_task,\n",
    "    tatqa_task\n",
    "]\n",
    "\n",
    "qrels = [\n",
    "    get_evalset('ConvFinQA'),\n",
    "    get_evalset('FinanceBench'),\n",
    "    get_evalset('FinDER'),\n",
    "    get_evalset('FinQA'),\n",
    "    get_evalset('FinQABench'),\n",
    "    get_evalset('MultiHeirtt'),\n",
    "    get_evalset('TATQA')\n",
    "]\n",
    "\n",
    "# for qrel, task in zip(qrels, tasks):\n",
    "#     metrics = task.evaluate(qrels=qrel, results=task.retrieve_results, k_values=[10])\n",
    "#     retrieve_ndcgs.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.tasks.Base_Task:NDCG@10: 0.1238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n",
      "0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 26\u001b[0m\n\u001b[1;32m     19\u001b[0m dataset_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvFinQA\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinanceBench\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m ]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qrel, task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(qrels, tasks):\n\u001b[0;32m---> 26\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mevaluate(qrels\u001b[38;5;241m=\u001b[39mqrel, results\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mretrieve_results, k_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m     28\u001b[0m     ndcg_values\u001b[38;5;241m.\u001b[39mappend(metrics[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNDCG@10\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# NDCG@10\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     map_values\u001b[38;5;241m.\u001b[39mappend(metrics[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAP@10\u001b[39m\u001b[38;5;124m'\u001b[39m])    \u001b[38;5;66;03m# MAP@10\u001b[39;00m\n",
      "File \u001b[0;32m/data/DSC/Financerag/financerag/tasks/Base_Task.py:451\u001b[0m, in \u001b[0;36mBaseTask.evaluate\u001b[0;34m(qrels, results, k_values, ignore_identical_ids)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[1;32m    450\u001b[0m     ndcg[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(ndcg[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m--> 451\u001b[0m     _map[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(_map[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    452\u001b[0m     recall[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(recall[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    453\u001b[0m     precision[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(precision[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ndcg_values = []\n",
    "map_values = []\n",
    "recall_values = []\n",
    "precision_values = []\n",
    "\n",
    "tasks = [\n",
    "    convfinqa_task,\n",
    "    finbench_task\n",
    "\n",
    "]\n",
    "\n",
    "qrels = [\n",
    "    get_evalset('ConvFinQA'),\n",
    "    get_evalset('FinanceBench')\n",
    "]\n",
    "\n",
    "dataset_names = [\n",
    "    'ConvFinQA',\n",
    "    'FinanceBench'\n",
    "]\n",
    "\n",
    "for qrel, task in zip(qrels, tasks):\n",
    "\n",
    "    metrics = task.evaluate(qrels=qrel, results=task.retrieve_results, k_values=[10])\n",
    "\n",
    "    ndcg_values.append(metrics[0]['NDCG@10'])  # NDCG@10\n",
    "    map_values.append(metrics[1]['MAP@10'])    # MAP@10\n",
    "    recall_values.append(metrics[2]['Recall@10'])  # Recall@10\n",
    "    precision_values.append(metrics[3]['P@10'])  # P@10\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(ndcg_values, marker='o', label='NDCG@10', color='b')\n",
    "plt.title('NDCG@10')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.tasks.Base_Task:NDCG@10: 0.3429\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m      8\u001b[0m dataset_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvFinQA\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinanceBench\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTATQA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m ]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qrel, task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(qrels, tasks):\n\u001b[0;32m---> 20\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mevaluate(qrels\u001b[38;5;241m=\u001b[39mqrel, results\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mretrieve_results, k_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m     22\u001b[0m     ndcg_values\u001b[38;5;241m.\u001b[39mappend(metrics[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNDCG@10\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# NDCG@10\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     map_values\u001b[38;5;241m.\u001b[39mappend(metrics[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAP@10\u001b[39m\u001b[38;5;124m'\u001b[39m])    \u001b[38;5;66;03m# MAP@10\u001b[39;00m\n",
      "File \u001b[0;32m/data/DSC/Financerag/financerag/tasks/Base_Task.py:448\u001b[0m, in \u001b[0;36mBaseTask.evaluate\u001b[0;34m(qrels, results, k_values, ignore_identical_ids)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# Compute the average scores for each k\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[0;32m--> 448\u001b[0m     ndcg[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(ndcg[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    449\u001b[0m     _map[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(_map[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    450\u001b[0m     recall[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(recall[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ndcg_values = []\n",
    "map_values = []\n",
    "recall_values = []\n",
    "precision_values = []\n",
    "\n",
    "dataset_names = [\n",
    "    'ConvFinQA',\n",
    "    'FinanceBench',\n",
    "    'FinDER',\n",
    "    'FinQA',\n",
    "    'FinQABench',\n",
    "    'MultiHeirtt',\n",
    "    'TATQA'\n",
    "]\n",
    "\n",
    "for qrel, task in zip(qrels, tasks):\n",
    "\n",
    "    metrics = task.evaluate(qrels=qrel, results=task.retrieve_results, k_values=[10])\n",
    "\n",
    "    ndcg_values.append(metrics[0]['NDCG@10'])  # NDCG@10\n",
    "    map_values.append(metrics[1]['MAP@10'])    # MAP@10\n",
    "    recall_values.append(metrics[2]['Recall@10'])  # Recall@10\n",
    "    precision_values.append(metrics[3]['P@10'])  # P@10\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(ndcg_values, marker='o', label='NDCG@10', color='b')\n",
    "plt.title('NDCG@10')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(map_values, marker='o', label='MAP@10', color='g')\n",
    "plt.title('MAP@10')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(recall_values, marker='o', label='Recall@10', color='r')\n",
    "plt.title('Recall@10')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(precision_values, marker='o', label='P@10', color='c')\n",
    "plt.title('Precision@10')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"jinaai/jina-reranker-v2-base-multilingual\",\n",
    "            \"Alibaba-NLP/gte-multilingual-reranker-base\",\n",
    "        \"BAAI/bge-reranker-v2-m3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Initialize CrossEncoder Reranker\n",
    "# base_reranker1 = \"jinaai/jina-reranker-v2-base-multilingual\"  # Model name\n",
    "# reranker1 = CrossEncoderReranker(\n",
    "#     model=CrossEncoder(base_reranker1, trust_remote_code=True)  # Ensure trust_remote_code is enabled\n",
    "# )\n",
    "\n",
    "base_reranker = \"BAAI/bge-reranker-v2-m3\" #\"BAAI/bge-reranker-base\" #m2 \n",
    "\n",
    "reranker = CrossEncoderReranker(\n",
    "    model=CrossEncoder(base_reranker)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Reranking (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on ConvFinQA Reranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-100....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3644d3f0404b1fbcc3bb511e9a64a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-100....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinBench Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b35e31c1854bec8a71da388585d038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinDER Reranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-100....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa0d94a475548b287a8fe3f52f9d035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinQA Reranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-100....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7559b01983f7424881ddd43f1e29f4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-100....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinQABench Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937613e0735a4d98b3afa14b326f9fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on MultiHiertt Reranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-100....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93f3f34883b44b99b8eb989abc72453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on TATQA Reranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-100....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704253d61443435abc6a1bc97d00e19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20788 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: Perform rerankin\n",
    "\n",
    "top_k= 100 # Number of Reranking results\n",
    "batch_size = 8 # 32\n",
    "\n",
    "print(\"\\nWorking on ConvFinQA Reranking\")\n",
    "convfinqa_rerank = convfinqa_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=convfinqa_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinBench Reranking\")\n",
    "finbench_rerank = finbench_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=finbench_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinDER Reranking\")\n",
    "finder_rerank = finder_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=finder_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinQA Reranking\")\n",
    "finqa_rerank = finqa_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=finqa_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinQABench Reranking\")\n",
    "finqabench_rerank = finqabench_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=finqabench_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on MultiHiertt Reranking\")\n",
    "multih_rerank = multih_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=multih_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on TATQA Reranking\")\n",
    "tatqa_rerank = tatqa_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=tatqa_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "reranking_results = [\n",
    "    convfinqa_rerank,\n",
    "    finbench_rerank,\n",
    "    finder_rerank,\n",
    "    finqa_rerank,\n",
    "    finqabench_rerank,\n",
    "    multih_rerank,\n",
    "    tatqa_rerank\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, rerank_result in enumerate(reranking_results):\n",
    "#     print(f\"\\nReranking results for Task {i + 1} ({len(rerank_result)} queries). Here's an example of the top 5 documents for the first query:\")\n",
    "\n",
    "#     for q_id, result in rerank_result.items():\n",
    "#         print(f\"\\nQuery ID: {q_id}\")\n",
    "        \n",
    "#         sorted_results = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#         for i, (doc_id, score) in enumerate(sorted_results[:5]):\n",
    "#             print(f\"  Document {i + 1}: Document ID = {doc_id}, Score = {score}\")\n",
    "\n",
    "#         break  # Only show the first query for each task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.tasks.Base_Task:NDCG@10: 0.4805\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m\n\u001b[1;32m     17\u001b[0m dataset_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvFinQA\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinanceBench\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTATQA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     25\u001b[0m ]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qrel, task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(qrels, tasks):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# 평가지표 평가 (retrieval 결과)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#retrieval_metrics = task.evaluate(qrels=qrel, results=task.retrieve_results, k_values=[10])\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# 평가지표 평가 (rerank 결과)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     rerank_metrics \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mevaluate(qrels\u001b[38;5;241m=\u001b[39mqrel, results\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mrerank_results, k_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# retrieval_ndcg_values.append(retrieval_metrics[0]['NDCG@10'])\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# retrieval_map_values.append(retrieval_metrics[1]['MAP@10'])\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# retrieval_recall_values.append(retrieval_metrics[2]['Recall@10'])\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# retrieval_precision_values.append(retrieval_metrics[3]['P@10'])\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     rerank_ndcg_values\u001b[38;5;241m.\u001b[39mappend(rerank_metrics[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNDCG@10\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/data/DSC/Financerag/financerag/tasks/Base_Task.py:448\u001b[0m, in \u001b[0;36mBaseTask.evaluate\u001b[0;34m(qrels, results, k_values, ignore_identical_ids)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# Compute the average scores for each k\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[0;32m--> 448\u001b[0m     ndcg[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(ndcg[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    449\u001b[0m     _map[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(_map[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    450\u001b[0m     recall[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(recall[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 각 평가지표를 저장할 리스트\n",
    "retrieval_ndcg_values = []\n",
    "rerank_ndcg_values = []\n",
    "\n",
    "retrieval_map_values = []\n",
    "rerank_map_values = []\n",
    "\n",
    "retrieval_recall_values = []\n",
    "rerank_recall_values = []\n",
    "\n",
    "retrieval_precision_values = []\n",
    "rerank_precision_values = []\n",
    "\n",
    "# 데이터셋 이름 리스트\n",
    "dataset_names = [\n",
    "    'ConvFinQA',\n",
    "    'FinanceBench',\n",
    "    'FinDER',\n",
    "    'FinQA',\n",
    "    'FinQABench',\n",
    "    'MultiHeirtt',\n",
    "    'TATQA'\n",
    "]\n",
    "\n",
    "for qrel, task in zip(qrels, tasks):\n",
    "    # 평가지표 평가 (retrieval 결과)\n",
    "    #retrieval_metrics = task.evaluate(qrels=qrel, results=task.retrieve_results, k_values=[10])\n",
    "    \n",
    "    # 평가지표 평가 (rerank 결과)\n",
    "    rerank_metrics = task.evaluate(qrels=qrel, results=task.rerank_results, k_values=[10])\n",
    "\n",
    "    # retrieval_ndcg_values.append(retrieval_metrics[0]['NDCG@10'])\n",
    "    # retrieval_map_values.append(retrieval_metrics[1]['MAP@10'])\n",
    "    # retrieval_recall_values.append(retrieval_metrics[2]['Recall@10'])\n",
    "    # retrieval_precision_values.append(retrieval_metrics[3]['P@10'])\n",
    "    \n",
    "    rerank_ndcg_values.append(rerank_metrics[0]['NDCG@10'])\n",
    "    rerank_map_values.append(rerank_metrics[1]['MAP@10'])\n",
    "    rerank_recall_values.append(rerank_metrics[2]['Recall@10'])\n",
    "    rerank_precision_values.append(rerank_metrics[3]['P@10'])\n",
    "\n",
    "# 그래프 생성\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# NDCG@10 시각화\n",
    "#plt.subplot(2, 2, 1)\n",
    "#plt.plot(retrieval_ndcg_values, marker='o', label='Retrieval NDCG@10', color='b')\n",
    "plt.plot(rerank_ndcg_values, marker='x', label='Rerank NDCG@10', color='b', linestyle='--')\n",
    "plt.title('NDCG@10 Comparison')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# # MAP@10 시각화\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.plot(retrieval_map_values, marker='o', label='Retrieval MAP@10', color='g')\n",
    "# plt.plot(rerank_map_values, marker='x', label='Rerank MAP@10', color='g', linestyle='--')\n",
    "# plt.title('MAP@10 Comparison')\n",
    "# plt.xlabel('Dataset')\n",
    "# plt.ylabel('Score')\n",
    "# plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# # Recall@10 시각화\n",
    "# plt.subplot(2, 2, 3)\n",
    "# plt.plot(retrieval_recall_values, marker='o', label='Retrieval Recall@10', color='r')\n",
    "# plt.plot(rerank_recall_values, marker='x', label='Rerank Recall@10', color='r', linestyle='--')\n",
    "# plt.title('Recall@10 Comparison')\n",
    "# plt.xlabel('Dataset')\n",
    "# plt.ylabel('Score')\n",
    "# plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# # Precision@10 시각화\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.plot(retrieval_precision_values, marker='o', label='Retrieval P@10', color='c')\n",
    "# plt.plot(rerank_precision_values, marker='x', label='Rerank P@10', color='c', linestyle='--')\n",
    "# plt.title('Precision@10 Comparison')\n",
    "# plt.xlabel('Dataset')\n",
    "# plt.ylabel('Score')\n",
    "# plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Reranking (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Initialize CrossEncoder Reranker\n",
    "base_reranker2 = \"BAAI/bge-reranker-v2-m3\" #\"BAAI/bge-reranker-v2-m3\" #\"jinaai/jina-reranker-v2-base-multilingual\" #'cross-encoder/ms-marco-MiniLM-L-12-v2' #\"BAAI/bge-reranker-base\"\n",
    "\n",
    "reranker2 = CrossEncoderReranker(\n",
    "    model=CrossEncoder(base_reranker2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-20....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on ConvFinQA Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a557f5614e24c6288a3dd89f4a951ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-20....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinBench Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f3ba99e3144a1789a3f531f5a2025f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-20....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinDER Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db8440bb8f743959e3af54cc82b534a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-20....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinQA Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa7eda400b44cba8aeeb92f4c14ee83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-20....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinQABench Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defce672ec2a4eb192b92802ecb39620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on MultiHiertt Reranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-20....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58192c89a1864fb49ad98dfb34403869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-20....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on TATQA Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e94929aca0477da05334b64a56429a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: Perform reranking\n",
    "\n",
    "top_k= 20 # Number of Reranking results\n",
    "batch_size = 8 # 32\n",
    "\n",
    "print(\"\\nWorking on ConvFinQA Reranking\")\n",
    "convfinqa_rerank_second = convfinqa_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=convfinqa_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinBench Reranking\")\n",
    "finbench_rerank_second = finbench_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=finbench_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinDER Reranking\")\n",
    "finder_rerank_second = finder_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=finder_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinQA Reranking\")\n",
    "finqa_rerank_second = finqa_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=finqa_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinQABench Reranking\")\n",
    "finqabench_rerank_second = finqabench_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=finqabench_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on MultiHiertt Reranking\")\n",
    "multih_rerank_second = multih_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=multih_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on TATQA Reranking\")\n",
    "tatqa_rerank_second = tatqa_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=tatqa_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "reranking_results_second = [\n",
    "    convfinqa_rerank_second,\n",
    "    finbench_rerank_second,\n",
    "    finder_rerank_second,\n",
    "    finqa_rerank_second,\n",
    "    finqabench_rerank_second,\n",
    "    multih_rerank_second,\n",
    "    tatqa_rerank_second\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.tasks.Base_Task:NDCG@10: 0.5377\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 19\u001b[0m\n\u001b[1;32m      8\u001b[0m dataset_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvFinQA\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinanceBench\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTATQA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m ]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qrel, task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(qrels, tasks):\n\u001b[0;32m---> 19\u001b[0m     rerank_second_metrics \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mevaluate(qrels\u001b[38;5;241m=\u001b[39mqrel, results\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mrerank_results, k_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m     20\u001b[0m     rerank_second_ndcg_values\u001b[38;5;241m.\u001b[39mappend(rerank_second_metrics[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNDCG@10\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     21\u001b[0m     rerank_second_map_values\u001b[38;5;241m.\u001b[39mappend(rerank_second_metrics[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAP@10\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/data/DSC/Financerag/financerag/tasks/Base_Task.py:441\u001b[0m, in \u001b[0;36mBaseTask.evaluate\u001b[0;34m(qrels, results, k_values, ignore_identical_ids)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m# Compute the average scores for each k\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[0;32m--> 441\u001b[0m     ndcg[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(ndcg[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    442\u001b[0m     _map[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(_map[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    443\u001b[0m     recall[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(recall[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rerank_second_ndcg_values = []\n",
    "rerank_second_map_values = []\n",
    "rerank_second_recall_values = []\n",
    "rerank_second_precision_values = []\n",
    "\n",
    "dataset_names = [\n",
    "    'ConvFinQA',\n",
    "    'FinanceBench',\n",
    "    'FinDER',\n",
    "    'FinQA',\n",
    "    'FinQABench',\n",
    "    'MultiHeirtt',\n",
    "    'TATQA'\n",
    "]\n",
    "\n",
    "for qrel, task in zip(qrels, tasks):\n",
    "    rerank_second_metrics = task.evaluate(qrels=qrel, results=task.rerank_results, k_values=[10])\n",
    "    rerank_second_ndcg_values.append(rerank_second_metrics[0]['NDCG@10'])\n",
    "    rerank_second_map_values.append(rerank_second_metrics[1]['MAP@10'])\n",
    "    rerank_second_recall_values.append(rerank_second_metrics[2]['Recall@10'])\n",
    "    rerank_second_precision_values.append(rerank_second_metrics[3]['P@10'])\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(retrieval_ndcg_values, marker='o', label='Retrieval NDCG@10', color='b')\n",
    "plt.plot(rerank_ndcg_values, marker='x', label='Rerank NDCG@10', color='b', linestyle='--')\n",
    "plt.plot(rerank_second_ndcg_values, marker='s', label='Rerank Second NDCG@10', color='b', linestyle='-.')\n",
    "plt.title('NDCG@10 Comparison')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(retrieval_map_values, marker='o', label='Retrieval MAP@10', color='g')\n",
    "plt.plot(rerank_map_values, marker='x', label='Rerank MAP@10', color='g', linestyle='--')\n",
    "plt.plot(rerank_second_map_values, marker='s', label='Rerank Second MAP@10', color='g', linestyle='-.')\n",
    "plt.title('MAP@10 Comparison')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(retrieval_recall_values, marker='o', label='Retrieval Recall@10', color='r')\n",
    "plt.plot(rerank_recall_values, marker='x', label='Rerank Recall@10', color='r', linestyle='--')\n",
    "plt.plot(rerank_second_recall_values, marker='s', label='Rerank Second Recall@10', color='r', linestyle='-.')\n",
    "plt.title('Recall@10 Comparison')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(retrieval_precision_values, marker='o', label='Retrieval P@10', color='c')\n",
    "plt.plot(rerank_precision_values, marker='x', label='Rerank P@10', color='c', linestyle='--')\n",
    "plt.plot(rerank_second_precision_values, marker='s', label='Rerank Second P@10', color='c', linestyle='-.')\n",
    "plt.title('Precision@10 Comparison')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reranker 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Initialize CrossEncoder Reranker\n",
    "base_reranker3 = \"BAAI/bge-reranker-v2-m3\" #\"BAAI/bge-reranker-v2-m3\" #\"jinaai/jina-reranker-v2-base-multilingual\" #'cross-encoder/ms-marco-MiniLM-L-12-v2' #\"BAAI/bge-reranker-base\"\n",
    "\n",
    "reranker3 = CrossEncoderReranker(\n",
    "    model=CrossEncoder(base_reranker2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/DSC/Financerag'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.tasks.Base_Task:Output directory set to: ./financerag/results/submission_12020100/ConvFinQA\n",
      "INFO:financerag.tasks.Base_Task:Output directory set to: ./financerag/results/submission_12020100/FinanceBench\n",
      "INFO:financerag.tasks.Base_Task:Output directory set to: ./financerag/results/submission_12020100/FinDER\n",
      "INFO:financerag.tasks.Base_Task:Output directory set to: ./financerag/results/submission_12020100/FinQA\n",
      "INFO:financerag.tasks.Base_Task:Output directory set to: ./financerag/results/submission_12020100/FinQABench\n",
      "INFO:financerag.tasks.Base_Task:Output directory set to: ./financerag/results/submission_12020100/MultiHiertt\n",
      "INFO:financerag.tasks.Base_Task:Output directory set to: ./financerag/results/submission_12020100/TAT-QA\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Save results\n",
    "results_dir = './financerag/results/'\n",
    "subfolder = 'submission_12020100'\n",
    "\n",
    "output_dir = results_dir+subfolder\n",
    "convfinqa_task.save_results(output_dir=output_dir)\n",
    "finbench_task.save_results(output_dir=output_dir)\n",
    "finder_task.save_results(output_dir=output_dir)\n",
    "finqa_task.save_results(output_dir=output_dir)\n",
    "finqabench_task.save_results(output_dir=output_dir)\n",
    "multih_task.save_results(output_dir=output_dir)\n",
    "tatqa_task.save_results(output_dir=output_dir)\n",
    "\n",
    "csv_files = [\n",
    "    output_dir + '/ConvFinQA/results.csv',\n",
    "    output_dir + '/FinanceBench/results.csv',\n",
    "    output_dir + '/FinDER/results.csv',\n",
    "    output_dir + '/FinQA/results.csv',\n",
    "    output_dir + '/FinQABench/results.csv',\n",
    "    output_dir + '/MultiHiertt/results.csv',\n",
    "    output_dir + '/TAT-QA/results.csv'\n",
    "]\n",
    "\n",
    "results_df = [pd.read_csv(file) for file in csv_files]\n",
    "combined_df = pd.concat(results_df, ignore_index=False)\n",
    "combined_df.to_csv(output_dir+'/'+subfolder+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\God_YJ\\\\interns\\\\DS_COMP\\\\FinanceRAG-main'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>corpus_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qd496c6a0</td>\n",
       "      <td>dd4b92b32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qd496c6a0</td>\n",
       "      <td>dd4ba2a5a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qd496c6a0</td>\n",
       "      <td>dd4be1f98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qd496c6a0</td>\n",
       "      <td>dd4ba07d2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qd496c6a0</td>\n",
       "      <td>dd4ba02f0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    query_id  corpus_id\n",
       "0  qd496c6a0  dd4b92b32\n",
       "1  qd496c6a0  dd4ba2a5a\n",
       "2  qd496c6a0  dd4be1f98\n",
       "3  qd496c6a0  dd4ba07d2\n",
       "4  qd496c6a0  dd4ba02f0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv('../data/sample_submission.csv')\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id present only in sample:\n",
      "['query_id']\n",
      "query_id present only in combined_df:\n",
      "['q00097', 'qd2abb228', 'q00029']\n"
     ]
    }
   ],
   "source": [
    "# sample과 combined_df에서 고유한 query_id 값 얻기\n",
    "sample_query_ids = sample['query_id'].unique()\n",
    "combined_query_ids = combined_df['query_id'].unique()\n",
    "\n",
    "# sample과 combined_df에만 존재하는 query_id 값 구하기\n",
    "sample_only_query_ids = list(set(sample_query_ids) - set(combined_query_ids))\n",
    "combined_only_query_ids = list(set(combined_query_ids) - set(sample_query_ids))\n",
    "\n",
    "# 결과 출력\n",
    "print(\"query_id present only in sample:\")\n",
    "print(sample_only_query_ids)\n",
    "\n",
    "print(\"query_id present only in combined_df:\")\n",
    "print(combined_only_query_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9677683,
     "sourceId": 85594,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DSC",
   "language": "python",
   "name": "dsc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
